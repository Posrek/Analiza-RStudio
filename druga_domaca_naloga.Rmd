---
title: "Druga domaca naloga"
output: html_document
date: "2023-09-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Ioann Stankovic
Prikaz pripravljenih podatkov za analizo:


```{r warning=FALSE}

library(readxl)
stanovanja <- read_excel("Apartments.xlsx")
head(stanovanja)

```

## Razlaga podatkov:

Torej imamo podatke za:

* Age, kar predstavlja koliko je stanovanje staro v letih.
* Distance, kar predstavlja razdaljo stanovanja od centra v kilometrih.
* Price, kar predstavlja ceno stanovanja na kvadratni meter v evrih, to bo tudi naša preučevana spremenljivka.
* Parking, predstavlja če stanovanje ima partking (kar označimo z 1) ali nima parkinga (označimo z 0).
* Balcony, predstavlja če stanovanje ima balkon (kar označimo z 1) ali nima balkona (označimo z 0).

Na podlagi naših podatkov je edina smiselna enota preučevanje cena stanovanja, predvidevamo da bo ta odvisna od starosti stanovanja, razdalje od centra in od tega ali ima uporabnik stanovanja dostop do balkona ali parkinga (to bomo tudi ugotovili z nadaljno analizo).

## Vir:
Podatki pridobljeni od profesorja.

## Cilj analize:
Napraviti smiselen linearen regresijski model za ceno stanovanja na kvadratni meter in pojasniti učinek regresijskih koeficientov v modelu.

## Opredelitev modela multiple regresije:

Torej naša odvisna spremenljivka y<sub>i</sub> modela multiple regresije bo cena stanovanja, pojasnjevalne spremenljivke x<sub>ij</sub> bodo starost stanovanja, razdalja stanovanja od centra, in indikatorski spremenljivki parking in balkon, ki predstavljata če ima stanovanje parking in/ali če ima stanovanje balkon. Regresijska enačba, ki bo določala naš model:

Price = β₀ + β₁ * Age + β₂ * Distance + β₃ * Balcony + β₄ * Parking + ε

## Grafični prikaz med izbranimi spremenljivkami

Napravimo razsevni diagram za vse pojasnjevalne spremenljivke z ocenjevano spremenljivko:

```{r}
library(ggplot2)


library(ggpubr)

plot_colors <- c("darkgoldenrod1", "deepskyblue3", "seagreen3", "sienna3")

plot1 <- ggplot(stanovanja, aes(x = Age, y = Price)) +
  geom_point(color = plot_colors[1]) +
  labs(x = "Leta", y = "Cena") +
  ggtitle("Razsevni diagram starosti stanovanja in cene")

plot2 <- ggplot(stanovanja, aes(x = Distance, y = Price)) +
  geom_point(color = plot_colors[2]) +
  labs(x = "Razdalja od centra", y = "Cena") +
  ggtitle("Razsevni diagram razdalje od centra in cene")

plot3 <- ggplot(stanovanja, aes(x = Balcony, y = Price)) +
  geom_point(color = plot_colors[3]) +
  labs(x = "Balkon", y = "Cena") +
  ggtitle("Razsevni diagram balkona in cene")

plot4 <- ggplot(stanovanja, aes(x = Parking, y = Price)) +
  geom_point(color = plot_colors[4]) +
  labs(x = "Parking", y = "Cena") +
  ggtitle("Razsevni diagram parkinga in cene")

ggarrange(plot1, plot2, plot3, plot4, ncol = 2, nrow = 2)

```

Nekako iz grafov lahko sklepamo da bližje kot smo centru bo v povprečju višja cena stanovanja (negativna korelacija), lahko tudi sklepamo, da če stanovanje ima parking potem bo imelo v povprečju višjo ceno (pozitivna korelacija).

## Ocena regresijske funkcije po metodi najmanjših kvadratov


```{r}
lm(Price ~ Age + Distance + Balcony + Parking, data = stanovanja)

model <- lm(Price ~ Age + Distance + Balcony + Parking, data = stanovanja)
summary(model)
```
Torej, če bodo vse pojasnjevalne spremenljivke 0 (novo stanovanje v centru, vendar brez balkona in parkinga) bo po oceni z metodo najmanjših kvadratov stanovanje stalo 2301.667 evrov na kvadratni meter, če je stanovanje starejše za eno leto se bo v povprečju cena stanovanja zmanjšala za 6.799 evrov na kvadratni meter, za vsak dodaten kilometer od centra bo stanovanje v povprečju cenejše za 18.045 evrov na kvadratni meter, če ima stanovanje balkon bo v povprečju dražje za 1.935 evrov na kvadratni meter in če ima stanovanje parking bo dražje za 196.168 evrov na kvadratni meter.

Če pogledamo p-vrednost naših pojasnjevalnih spremenljivk, vidimo da so vse manjše od 0.05 razen pojasnjevalne spremenljivke za balkon, ki je 0.97436, kar pomeni da ta spremenljivka ni statistično značilna za naš model. Torej jo lahko odstranimo.

## Izboljšan regresijski model za preučevano slučajno spremenljivko

```{r}

lm(Price ~ Age + Distance + Parking, data = stanovanja)

model2 <- lm(Price ~ Age + Distance + Parking, data = stanovanja)

summary(model2)

```

## Predpostavke regresijskega modela

Poglejmo če so izpolnjene predpostavke obeh regresijskih modelov:

1. Linearnost regresijskega modela, je izpolnjena.

2. Zahteva da so vrednosti pojasnjevalnih spremenljivk fiksne (nestohastične) pri ponovitvah vzorcev, je izpolnjena.

3. Predpostavka o ničelni povprečni vrednosti residualov, bi morala biti izpolnjena, poglejmo.


```{r}
mean(resid(model))
mean(resid(model2))

```

Vrednost je zelo blizu 0.

Preden naredimo preizkus ali ima residual upanje res enako 0, preverimo če je residual porazdeljen normalno. Postavimo naslednjo hipotezo za oba modela:

* H<sub>0</sub> : Residual je porazdeljen normalno.
* H<sub>1</sub> : Residual ni porazdeljen normalno.

Hipotezo preverimo s Shapirov-Wilkovim testom.

```{r}
shapiro.test(resid(model))
shapiro.test(resid(model2))

```

Pri obeh preizkusih dobimo p-vrednost zelo majhno (<0.05), hipotezo H<sub>0</sub> zavrnemo in nemoremo sklepati, da sta residuala porazdeljena normalno.

Poglejmo si še kako izgledata histogram residualov:

```{r}
hist(resid(model),
     xlab = "Vrednost residuala",
     ylab = "Frekvenca",
     main = "Histogram residualov",
     col = "sienna1",
     ylim = range(0,15),
     xlim = range(-500,700))

hist(resid(model2),
     xlab = "Vrednost residuala",
     ylab = "Frekvenca",
     main = "Histogram residualov",
     col = "turquoise4",
     ylim = range(0,15),
     xlim = range(-500,700))

```

Histograma izgledata približno normalno porazdeljena.

Naredimo t-test za oba modela za naslednjo hipotezo:

* H<sub>0</sub> : Residual ima pričakovano vrednosto enako nič, E(ε) $=$ 0.
* H<sub>1</sub> : E(ε) $\neq$ 0.

```{r}

t.test(resid(model), mu = 0)
t.test(resid(model2), mu = 0)

```

Pri obeh preizkusih dobimo p-vrednost = 1 (>0.05), kar implicira da je upanje res 0, hipotezo H<sub>0</sub> ne zavrnemo. Predpostavimo, da je tretja predpostavka izpolnjena.

4. Predpostavka o homoskedastičnost

Preverimo, če imamo v našem modelu homoskedastičnost, postavimo ničelno hipotezo:

* H<sub>0</sub> : Varianca je konstantna (model je homoskedastičen).
* H<sub>1</sub> : Varianca ni konstantna (model je heteroskedastičen).

```{r warning=FALSE,message = FALSE}
# Naložimo install.packages("lmtest")
library(lmtest)

bptest(model)
bptest(model2)

```

V obeh primerih dobimo p-vrednost > 0.05 in ničelno hipotezo ne zavrnemo. Predostavimo, da je naš model homoskedastičen.

5. Odsotnost avtokorelacije

Narišimo razsevni diagram residualov, na podlagi katerega bomo videli avtokorealcijo.

```{r}

plot(residuals(model), xlab = "opazovanje", ylab = "residual", main = "Razsevni diagram residualov")
plot(residuals(model2), xlab = "opazovanje", ylab = "residual", main = "Razsevni diagram residualov")

```

Razsevni diagram nakazuje, da ni avtokorelacije.

6. Nekoreliranost med pojasnjevalnimi spremenljivkami in slučajno spremenljivko residuala

```{r}
residuals <- resid(model)

residuals2 <- resid(model2)

cov(residuals, stanovanja[, c("Age", "Balcony", "Distance", "Parking")])

cov(residuals2, stanovanja[, c("Age", "Distance", "Parking")])

```

Vrednosti kovariance so zelo majhne (blizu 0), zato sklepamo da je tudi ta predpostavka izpolnjena.

7. Število opazovanj presega število pojasnjevalnih spremenljivk, torej tudi ta predpostavka je izpolnjena

8. Variabilnost vrednosti pojasnjevalnih spremenljivk mora biti končno pozitivno število.

```{r}

variance_Age <- var(stanovanja$Age)
variance_Distance <- var(stanovanja$Distance)
variance_Balcony <- var(stanovanja$Balcony)
variance_Parking <- var(stanovanja$Parking)

print(c(variance_Age,variance_Balcony,variance_Distance,variance_Parking))

```

Tudi ta predpostavka je izpolnjena.

9. Predpostavimo tudi, da je naš model pravilno specificiran.

10. Preverimo odsotnost popolne multikolinearnosti.

```{r warning=FALSE, message=FALSE}

cor(stanovanja[, c("Age", "Distance", "Balcony" , "Parking")])

library(car)

vif(model)

vif(model2)

```

Na podlagi statistike VIF ugotavljamo, da v konkretnem primeru ni težave s premočno multikolinearnostjo, saj so vse vrednosti pod kritično mejo 5.

Vidimo, da je korelacija med prisotnostjo parkirišča in razdaljo od centra dokaj nizka (-0.307), kar implicira negativno korelacijo med razdaljo od centra in prisotnostjo parkirišča to pomeni, da bolj, ko smo oddaljeni od centra večja je verjetnost, da imamo na razpolago parkirišče, vendar je ta vrednost še vedno zelo majhna, zato lahko sklepamo, da je ta predpostavka tudi izpolnjena.

11. Iz podatkov odstranimo potencialne osamelce z visokim vplivom na ocenjevano regresijsko funkcijo.

```{r}

ostanki <- round(rstandard(model), 3)

cookd <- round(cooks.distance(model), 3)

print(cookd)

hist(ostanki,
     xlab = "Standardizirani ostanki",
     ylab = "Frekvenca",
     main = "Histogram standardiziranih ostankov",
     col = "slateblue3",
     xlim = range(-3,3))

ostanki2 <- round(rstandard(model2), 3)

cookd2 <- round(cooks.distance(model2), 3)

print(cookd2)

hist(ostanki2,
     xlab = "Standardizirani ostanki",
     ylab = "Frekvenca",
     main = "Histogram standardiziranih ostankov",
     col = "slateblue3",
     xlim = range(-3,3))

```

Ostanki obeh modelov se porazdeljujejo približno normalno, vsi se nahajajo znotraj intervala [-3,3]. V konkretnem primeru nimamo težav z osamelci.

```{r}

hist(cookd,
     xlab = "Cookove razdalje",
     ylab = "Frekvenca",
     main = "Histogram Cookovih razdalj",
     col = "slateblue3")


hist(cookd2,
     xlab = "Cookove razdalje",
     ylab = "Frekvenca",
     main = "Histogram Cookovih razdalj",
     col = "slateblue3")

```

V analizi na podlagi Cookovih razdalj smo se odločili, da ne bomo odstranili enot iz modelov, saj so vse razdalje pod mejno vrednostjo 1.

Vse predpostavke pri obeh modelih so izpolnjene.

Odločimo se, da je drugi model bolj primeren model za naše podatke na podlagi sledečega:

* Drugi model ima višjo vrednost prilagojenega R-kvadrata kot prvi model, prilagojeni R-kvadrat upošteva število pojasnjevalnih spremenljivk v modelu in je manjši glede na vključitev manj pomembnih spremenljivk v model.
* Drugi model ima manjšo standardno napako residuala, kar pomeni da se drugi model bolje prilega podatkom.
* Drugi model ima večjo vrednost F-statistike kot prvi model in p-vrednost je v drugem modelu manjša kot v prvem. To pomeni da regresijski koeficienti dajejo boljšo napoved modela glede na podatke.
* p-vrednost za pojasnjevalno spremenljivko balkon je v prvem modelu >0.05, kar pomeni da glede na drugi model ni značilne statistične razlike.

## Prikaz ustreznega modela

```{r}

model2 <- lm(Price ~ Age + Distance + Parking, data = stanovanja)

summary(model2)

sqrt(summary(model2)$r.squared)

```

Torej za naš model veljajo naslednje stvari. Model predpostavlja, da ko so vse pojasnjevalne spremenljivke 0 (oddaljenost od centra 0, stanovanje staro 0 let, brez balkona in parkirišča), da cena stanovanje v povprečju stane 2302.675 evrov na kvadratni meter, za vsako leto ko je stanovanje starejše cena v povprečju pade za 6.805 evrov na kvadratni meter, za vsak dodaten kilometer od centra cena v povprečju stanovanja na kvadrtni meter pade za 18.046 evrov in če ima stanovanje možnost uporabe parkirišča cena stanovanja v povprečju zraste za 196.083 evrov.


## Opisna statistika podatkov uporabljenih v končnem modelu

```{r}

stanovanja$Parking <- as.factor(stanovanja$Parking)
stanovanja$Balcony <- as.factor(stanovanja$Balcony)
summary(stanovanja)

```

Povprečna cena stanovanja je 2019 evrov, najcenejše stanovanje je 1400 in najdražje je 2820 evorv, dokaj majhen razpon v ceni. Povprečna razdalja od centra je 14.22 kilometrov najmanjša razdalja je 1 kilometer in največja 45 kilometrov oddaljenosti od centra. Razmerje parkirišč in balkonov, ki ga stanovanja imajo je priblišno enako (v prvem primeru 42:43 in v drugem primeru 48:37).

## Razlaga koeficientov in analize

Parcialne koeficiente za naš model smo že razložili. Multipli determinacijski koeficient R-kvadrat, ki je 0.5004 nam pove koliko variabilnosti lahko razložimo v preučevani spremenljivki s pomočjo pojasnjevalnih spremenljivk, višja kot je vrednost boljša je razlaga variablinosti z modelom (boljši model imamo), v našem primeru je ta koeficient zadovoljivo visok (razloži približno 50.04% variance cene v modelu). F-statistika je v našem primeru 27.04, ta nam pove natančnost regersijskega modela. Večja vrednost F-statistike in manjša p-vrednost implicira, da vsaj ena pojansjevalna spremenljivka v modelu pomembno pripomore k razlagi variance. V našem primeru je F-statistika relativno velika in p-vrednost povezana z njo precej majhna, kar implicira, da je model statistično značilen. Na podlagi ocenjenega koeficienta multiple korelacije ugotavljamo, da je linearna povezanost med preučevano spremenljivko in pojasnjevalnimi močna (saj je vrednost koeficienta multiple korelacije = 0.7073978, ta vrednost se nahaj na intervalu [0,1]).